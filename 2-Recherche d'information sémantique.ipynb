{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcXMvEFMrmCy"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeignobosLouis/tp-information-retrieval-with-llm-student-version/blob/main/2-Recherche%20d'information%20s%C3%A9mantique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEbudnhlrmC8"
      },
      "source": [
        "# Partie 2. - Recherche d'Information sémantique\n",
        "\n",
        "Les recherches réalisées dans le TP précédent sont principalement des recherches par mots ou par phrases basés sur le modèle `tf-idf`. Ce dernier construit un espace vectoriel dont la taille est égal au nombre total de tokens distincts dans la collection de documents. L'image ci-dessous représente un espace vectoriel avec 3 tokens distincts. Imaginez ce que cela donnerait avec 100,000 tokens distincts !\n",
        "\n",
        "![tf-df-vector-space](https://github.com/SeignobosLouis/tp-information-retrieval-with-llm-student-version/blob/main/resources/tfidf-vector-space.png?raw=1)\n",
        "\n",
        "Certes, des techniques existent pour limiter l'impact des variations syntaxiques (bas/haut de casses, mots au pluriel/singulier, synonymes) mais cela pose plusieurs problèmes :\n",
        "- effort requis pour paramétrer minutieusement la construction des tokens ;\n",
        "- recourt à des dictionnaires, notamment pour les synonymes ;\n",
        "- prise en compte des points précédents pour différentes langues ;\n",
        "- sens d'une phrase, paragraphe, document non pris en compte dans sa globalité.\n",
        "\n",
        "Pour palier ces problèmes, on peut utiliser des techniques avancées de Traitement Automatique du Langage Naturel (TALN) pour construire des espaces vectoriels _sémantiques_ où les mots, paragraphes, documents sont représentés par des vecteurs, appelés _embeddings_, encodant le sens des informations plutôt que leur syntaxe. Les espace vectoriels associés ont une taille fixe, de quelques centaines de dimensions. Ci-dessous un exemple de ce type d'espace en 2 dimensions (source : https://dev.to/jinglescode/word-embeddings-16hb)\n",
        "\n",
        "![embeddings](https://github.com/SeignobosLouis/tp-information-retrieval-with-llm-student-version/blob/main/resources/embeddings_2d.png?raw=1)\n",
        "\n",
        "Grâce aux _modèles de langue_, notamment aux _transformers_ (https://arxiv.org/abs/1706.03762) pré-entraînés et proposés librement par des sociétés comme Google, OpenAI, Facebook, il est maintenant possible de construire ses propres _embeddings_ sur n'importe quel texte.\n",
        "\n",
        "Huggingface (https://huggingface.co/) est une plateforme proposant nombre de ces modèles pré-entraînés, en particulier des modèles de type _transformers_ que l'on peut utiliser sur nos propres données : https://huggingface.co/sentence-transformers.\n",
        "\n",
        "Dans ce TP, nous allons utiliser l'un de ces modèles pour construire un _embedding_ par document et nous permettre de faire des recherches sémantiques et de trouver des documents similaires.\n",
        "\n",
        "En sortie de ce module, vous serez capable de :\n",
        "\n",
        "- Calculer l'_embedding_ d'un texte, c'est à dire sa représentation sémantique. En fonction du modèle choisi pour calculer les embeddings, ces derniers peuvent même être multilangues !\n",
        "- Rechercher des documents de manière plus pertinentes grâce à la recherchs sémantique ;\n",
        "- Mettre en oeuvre un système de Question / Réponse en utilisant la méthologie _Retrieval Augmented Generation (RAG)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ou15guPrmDP"
      },
      "source": [
        "### Instruction à suivre pour exécution sur Google Colab\n",
        "\n",
        "Aller dans `Execution -> Modifier le type d'exécution` puis sélectionner `T4-GPU` pour exploiter les fonctionnalités GPU.\n",
        "\n",
        "![Colab GPU](resources/colab_gpu.png \"T4-GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRnVB67brmDV"
      },
      "source": [
        "### Import des bibliothèques logicielles et configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GAQbmuirmDi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Vérifie si le code est exécuté sur Google Colab\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    # Commandes à exécuter uniquement sur Google Colab\n",
        "    !git clone https://github.com/vincentmartin/tp-information-retrieval-with-llm-student-version.git\n",
        "    %cd tp-information-retrieval-with-llm-student-version\n",
        "    !pip install -r requirements.txt\n",
        "else:\n",
        "    # Commandes à exécuter si ce n'est pas sur Google Colab\n",
        "    print(\"Pas sur Google Colab, ces commandes ne seront pas exécutées.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ry8DriPQrmDo"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8757eTSrmEE"
      },
      "source": [
        "### Lecture des données\n",
        "\n",
        "Ce premier bloc permet de lire les données. Grâce à [langchain](https://python.langchain.com/docs/get_started/introduction), lire des données est très facile et cela ne requiert que quelques lignes de codes.\n",
        "\n",
        "Etudier la document [Document loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/) pour en apprendre plus sur la lecture des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbtg5GsArmEG",
        "outputId": "5fee4b92-9d7a-45a1-d699-6e0afb610c55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2225 documents lus.\n"
          ]
        }
      ],
      "source": [
        "directory = './data_bbc_news/'\n",
        "\n",
        "def load_docs(directory):\n",
        "  loader = DirectoryLoader(directory)\n",
        "  documents = loader.load()\n",
        "  return documents\n",
        "\n",
        "documents = load_docs(directory)\n",
        "print('{} documents lus.'.format(len(documents)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe68t-zWrmEI"
      },
      "source": [
        "*texte en italique*### Découpage des documents\n",
        "\n",
        "Les modèles _sentence transformers_ ne prennent qu'un nombre limité de tokens en entrée. C'est pourquoi il est nécessaire de découper les documents en plusieurs blocs. Ce découpage présente aussi l'avantage d'être plus précis dans la recherche et de ne cibler que les paragraphes les plus pertinents pour une recherche.\n",
        "\n",
        "Le bloc ci-dessous réalise ce découpage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HZ_m7njrmEJ",
        "outputId": "dece710c-574c-4554-896b-84c7d5b2f0ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2225 documents découpés en 2225 blocs.\n"
          ]
        }
      ],
      "source": [
        "def split_docs(documents,chunk_size=10000,chunk_overlap=20):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "  docs = text_splitter.split_documents(documents)\n",
        "  return docs\n",
        "\n",
        "paragraphs = split_docs(documents)\n",
        "print('{} documents découpés en {} blocs.'.format(len(documents), len(documents)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXbE4Up8rmE3"
      },
      "source": [
        "### Chargement du modèle _sentence transformer_\n",
        "\n",
        "La ligne ci-dessous permet de charger un modèle _sentence transformer_ permettant d'encoder un bloc de texte en un _embedding_, c'est à dire un vecteur de réels de taille fixe. Le modèle est automatiquement téléchargé sur la plateforme [Huggingface](https://huggingface.co).\n",
        "\n",
        "Jetez un oeil à la documentation des _sentence transformers_ ici : https://www.sbert.net/\n",
        "\n",
        "Certains modèle comme [celui-ci](https://huggingface.co/intfloat/multilingual-e5-base) sont multi langues.\n",
        "\n",
        "Vous pouvez remplacer le modèle par défaut par un autre modèle. Une liste de modèle est disponible ici https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs9qAqNRrmE6"
      },
      "source": [
        "#### Exercice 1\n",
        "\n",
        "Dans la liste des modèle disponible ici https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads :\n",
        "- Rechercher le modèle `intfloat/multilingual-e5-base`.\n",
        "- Enlisant la documentation sur la page, spécifier la valeur du paramètre `model_name` avec le modèle trouvé."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMdszvpCrmE8"
      },
      "outputs": [],
      "source": [
        "embeddings = SentenceTransformerEmbeddings(model_name=\"intfloat/multilingual-e5-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-xh3K5SrmFB"
      },
      "source": [
        "### Indexation des documents dans la BDD vectorielle _ChromaDB_\n",
        "\n",
        "[ChromaDB](https://www.trychroma.com/) est une base de données vectorielle, un _Vector Store_. Elle permet de stocker des _embeddings_ pour des textes mais aussi pour des images et des fichiers audios.\n",
        "\n",
        "La ligne ci-dessous permet d'indexer les paragraphes calculés ci-dessus dans la base ChromaDB en utilisant le modèle _sentence transformer_ préalablement chargé."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_lw4AHZ2rmFC"
      },
      "outputs": [],
      "source": [
        "chromadb_dir = \"./chromadb\" # Chemin où seront stockées les données\n",
        "db = Chroma.from_documents(paragraphs, embeddings, persist_directory=chromadb_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpQHlb19rmFD"
      },
      "source": [
        "### Interrogation\n",
        "\n",
        "Interroger les documents est une étape simple pour l'utilisateur, même si les mécanismes sous-jacents restent complexes.\n",
        "\n",
        "La ligne ci-dessous permet de rechercher les documents similaire à la requête `query`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUAJzBygrmFE"
      },
      "source": [
        "#### Exercice 2\n",
        "\n",
        "En lisant la documentation ici https://python.langchain.com/docs/integrations/vectorstores/chroma :\n",
        "- Rechercher les documents similaires à la requête `query`\n",
        "- Afficher les 5 documents les plus pertinents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHIWQN58rmFF"
      },
      "outputs": [],
      "source": [
        "query = \"Jeux vidéo Playstation\"\n",
        "retriever = db.as_retriever()\n",
        "matching_docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "for i, doc in enumerate(matching_docs[:5]):\n",
        "    print(f\"Document {i + 1}: {doc.page_content}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVp3S0_krmFH"
      },
      "source": [
        "### Chatbot en mode RAG\n",
        "\n",
        "Dans cette partie, nous mettons en oeuvre un chatbot en utilisant un _Large Language Model_ qui va se servir des documents trouvés dans la BDD vectorielle pour synthétiser et les informations et construire une réponse.\n",
        "\n",
        "Nous pourrions utiliser GPT3.5 ou GPT4 mais pour des raisons de coût (il faut un abonnement payant), nous allons utiliser un petit modèle open source [llama2](https://ai.meta.com/llama/).\n",
        "\n",
        "Nous allons même réaliser une petite IHM du chatbot avec la bibliothèque [gradio](https://www.gradio.app/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ehOemvormFI"
      },
      "source": [
        "#### Import des bibliothèques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QQM_5UnYrmFJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import transformers\n",
        "import gradio as gr\n",
        "\n",
        "from textwrap import fill\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    )\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
        "\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiOiLdi2rmFK"
      },
      "source": [
        "#### Configuration du LLM\n",
        "\n",
        "Dans cette partie, nous allons utiliser le modèle [llamav2](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf) dôté de 7 milliards de paramètres. C'est le plus petit des trois modèles de l'organisation puisqu'il y a ensuite les modèles de 13 et 70 milliards de paramètres.\n",
        "\n",
        "Le modèle est automatiquement téléchargé sur la plateforme [Huggingface](https://huggingface.co)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjLdlioRrmFK"
      },
      "outputs": [],
      "source": [
        "llm_model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Chargement de la configuration\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    llm_model_id\n",
        ")\n",
        "\n",
        "# Chargement du modèle LLM\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    llm_model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    device_map='auto',\n",
        "    load_in_8bit=True,\n",
        ")\n",
        "\n",
        "# Chargement du tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_model_id)\n",
        "\n",
        "# Configuration de la génération\n",
        "generation_config = GenerationConfig.from_pretrained(llm_model_id)\n",
        "generation_config.max_new_tokens = 1024\n",
        "generation_config.temperature = 0.0001 # plus la température est basse, plus les prédictions sont précises\n",
        "generation_config.top_p = 0.95\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "# Création du pipeline\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "# Création du pipeline LLM\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=pipeline,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPFHuK_ZrmFK"
      },
      "source": [
        "#### Création du _prompt template_\n",
        "\n",
        "Un prompt template permet de configurer la manière dont le LLM doit se comporter. Grâce à la bibliothèque [langchain](https://www.langchain.com/), la création d'un _prompt template_ est simplifiée et permet notamment d'inclure des variables quis seront spécifiées plus tard, lors de l'exécution de la chaîne.\n",
        "\n",
        "Dans l'exemple ci-dessous, le template est rédigé en anglais car la majeure partie des documents sur lesquels le LLM a été entraîné sont en anglais. La performance est meilleur qu'en français."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NUlPOlnrmFL"
      },
      "source": [
        "#### Exercice 3\n",
        "\n",
        "Modifier le `prompt_template` pour indiquer dans le bloc `[INST]` les instructions nécessaire pour que le LLM :\n",
        "- Agisse comme un journaliste\n",
        "- Répondre à la question en utilisant seulement les éléments du contexte et rien d'autres\n",
        "- Cite les paragraphes pertinents qui ont permis de répondre à la questions\n",
        "\n",
        "Ces instructions doivent être écrites en anglais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "dXevBXRErmFL"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "[INST]\n",
        "ChatGPT\n",
        "\n",
        "You are a French journalist.\n",
        "Answer questions in French, even if asked in another language.\n",
        "People will ask questions with a situation, and your answers should only come from that situation.\n",
        "Always mention specific parts of the given situation when responding.\n",
        "Only use English when quoting from the situation.\n",
        "If there's a question mark, you'll answer in French too.\n",
        "[/INST]\n",
        "\n",
        "[CONTEXT]\n",
        "{context}\n",
        "[/CONTEXT]\n",
        "\n",
        "[QUESTION]\n",
        "{question}\n",
        "[/QUESTION]\n",
        "\n",
        "[CITATION]\n",
        "[/CITATION]\n",
        "\n",
        "[RESPONSE]\n",
        "[/RESPONSE]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HleHOT89rmFL"
      },
      "source": [
        "#### Configuration de la chaîne de Question/Réponse\n",
        "\n",
        "Les lignes ci-dessous permettent de configurer le système de Question Réponse. Dans cet exemple, le système récupère les 2 paragraphes les plus pertinents (`\"k\":2`) pour construire la réponse. Il est possible d'augmenter ce nombre mais cela demandera plus de temps pour construire la réponse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "IEwP4grvrmFM"
      },
      "outputs": [],
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm, # LLM utilisé, en l'occrurence llamav2 7B\n",
        "    chain_type=\"stuff\", # Type de chaîne à utiliser\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 2}), # récupérateur des k documents les plus pertinents\n",
        "    return_source_documents=True, # retourne les documents sources\n",
        "    chain_type_kwargs={\"prompt\": prompt}, # prompt à utiliser\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPec2UNqrmFM"
      },
      "source": [
        "#### IHM du chatbot avec gradio\n",
        "\n",
        "Les quelques lignes ci-dessous permettent de construire l'IHM du chatbot qui sera accessible depuis un navigateur Web. Depuis google colab, un lien générique durant 24H est généré automatiquement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-fREGkArmFO"
      },
      "outputs": [],
      "source": [
        "def ask(question, history):\n",
        "    response = qa_chain(\n",
        "      question\n",
        "    )\n",
        "    return response[\"result\"].strip()\n",
        "\n",
        "interface = gr.ChatInterface(\n",
        "    fn = ask,\n",
        "    chatbot=gr.Chatbot(height=600),\n",
        "    textbox=gr.Textbox(placeholder=\"What is quantum computing?\", container=False, scale=7),\n",
        "    title=\"CNAM ChatBot\",\n",
        "    theme=\"soft\",\n",
        "    examples=[\"What is quantum computing?\"],\n",
        "\n",
        "    cache_examples=True,\n",
        "    retry_btn=\"Relancer\",\n",
        "    undo_btn=\"Annuler\",\n",
        "    clear_btn=\"Réinitialiser\",\n",
        "    submit_btn=\"Envoyer\"\n",
        "\n",
        "    )\n",
        "\n",
        "interface.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE3jTIxkrmFP"
      },
      "source": [
        "#### Exercice 4.\n",
        "\n",
        "En utilisant le chatbot construit, indiquer ci-dessous les réponses aux questions ci-après."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8fPzuI-rmFP"
      },
      "source": [
        "#### Quelques exemples de questions à poser\n",
        "\n",
        "- Who is Dr Mario Paniccia?\n",
        "Dr. Mario Paniccia is mentioned in the following passage:\"Home Secretary David Blunkett has spoken of his love for married publisher Kimberly Quinn for the first time. The home secretary described how it affected his friends and personal life, but said he was a great believer in personal responsibility. Mr Blunkett is taking legal action to gain access to Mrs Quinn's two-year-old son. She denies he is Mr Blunkett's. The interview with BBC Radio Sheffield was made before allegations he fast-tracked a visa for Mrs Quinn's nanny. The allegations, which he has denied, are being investigated by Sir Alan Budd.\"(Paragraph 4) Therefore, based on this passage, Dr. Mario Paniccia appears to be neither the husband nor the child of Kimberly Quinn, as the legal action taken by Mr. Blunkett pertains to gaining access to Mrs. Quinn's two-year-old son, rather than her husband.\n",
        "\n",
        "- What celebrity opens February's Super Bowl?\n",
        "According to the context, Alicia Keys will open February's Super Bowl by singing \"America the Beautiful.\" This information can be found in the following passage:\n",
        "\"R&B star Alicia Keys is to open February's Super Bowl singing a song only previously performed there by Ray Charles and Vicki Carr.\" (Paragraph 1)\n",
        "Additionally, the context mentions that Sir Paul McCartney will provide the halftime entertainment, and that organizers have promised there will be no repeat of last year's controversial performance by Janet Jackson. These details can be found in the following passages:\n",
        "\"Sir Paul McCartney will provide the half-time entertainment...Organizers have promised there will be no repeat of Janet Jackson's nipple-baring incident...\" (Paragraph 3)\n",
        "Overall, these passages provide the answer to the question regarding who will open the Super Bowl.\n",
        "\n",
        "- What is the primary objective of the upcoming meeting of military chiefs regarding Scotland's Army regiments?\n",
        "Based on the information provided in the context, the primary objective of the upcoming meeting of military chiefs regarding Scotland's Army regiments is to make a final decision on the future of Scotland's Army regiments. According to the passage, the committee of the Army Board will discuss plans for restructuring regiments on Monday, including cutting Scotland's six single-battalion regiments to five and merging these into a super regiment. The decision made by the committee must be ratified by Defence Secretary Geoff Hoon and Prime Minister Tony Blair, and is expected to be made public next week. (Paragraphs 1-4)\n",
        "\n",
        "\n",
        "- What injury did Chris Tomlinson sustain, and how has it affected his training and competition plans?\n",
        "According to the context, Kelly Holmes has suffered a hamstring injury that will keep her out of competition for two to three weeks. This information supports the answer to the question about Chris Tomlinson's injury. Specifically, Paragraphs 7 and 9 of the context mention Holmes' hamstring injury and its impact on her training and competition plans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRrLgiSPrmFR"
      },
      "source": [
        "#### Exercice 5.\n",
        "\n",
        "Modifier le `prompt_template` pour que le chatbot réponde en français.\n",
        "Voici les réponses automatiquement données en français avec les même questions de l'exercice 4 posées en anglais.\n",
        "\n",
        "- Who is Dr Mario Paniccia?\n",
        "Dr Mario Paniccia est mentionné dans le contexte comme l'avocat de Kimberly Quinn, l'ex-amante de David Blunkett. Il est cité plusieurs fois pour ses déclarations sur le cas et les accusations envers Blunkett.\n",
        "\n",
        "- What celebrity opens February's Super Bowl?\n",
        "Alicia Keys est la célèbre artiste qui ouvre le Super Bowl de février, accompagnée de 150 étudiants de l'école pour les aveugles et les malentendants de Floride. Elle interprète une chanson précédemment exécutée à ce événement par Ray Charles et Vicki Carr. Keys décrit Charles comme un artiste qu'elle admire, regrette et respecte. Elle affirme être très excitée pour cette prestation « touchante et mémorable ». C'est sa première participation au Super Bowl, qui sera suivi de plus de 144 millions de personnes aux États-Unis le 6 février. Sir Paul McCartney assurerá l'entertainment pendant la pause entre les deux parties du match. Les organisateurs ont promis que cette année, il n'y avoirait pas de récidive de l'incident de Janet Jackson qui a provoqué des milliers de plaintes aux États-Unis.\n",
        "\n",
        "- What is the primary objective of the upcoming meeting of military chiefs regarding Scotland's Army regiments?\n",
        "Les militaires chefs vont discuter d'une décision finale sur l'avenir des régiments de l'Armée écossaise. Le comité de l'Armée du Board est censé discuter des plans de réorganisation des régiments le lundi prochain. Les propositions incluent la suppression des six régiments écossais en une seule unité. Les plans ont rencontré une opposition féroce de la part des campagnistes et des politiciens. Le Comité a prendre une décision qui doit être approuvée par le Secrétaire d'État à la Défense Geoff Hoon et par le Premier Ministre Tony Blair. Il est attendu que cette décision soit rendue publique dans la semaine prochaine. Lorsque les ministres ont annoncé une réorganisation de l'Armée, cela a suscité des questions sur l'avenir des Black Watch, des Kings Own Scottish Borderers, des Royal Scots, des Royal Highland Fusiliers et des Argyll and Sutherland Highlanders. En octobre, le Conseil des colonels écossais avait proposé la fusion des Royal Scots et des King's Own Scottish Borderers dans un seul bataillon.\n",
        "Selon leur proposition, il y aurait un seul des cinq bataillons de super régiment. Les propositions pour soit réunir ou fusionner les six régiments en un seul bataillon ont provoqué une polémique politique, avec les travaillistes de base et les politiciens opposés au plan. Ils ont cru que le moment était inapproprié car les Black Watch étaient en première ligne en Irak, souffrant de blessures. Les Campagneurs pour sauver les régiments écossais ont été si offensés qu'ils ont menacé de se présenter contre les Travaillistes aux prochaines élections générales.\n",
        "Lequel est le principal objectif de la prochaine réunion des chefs militaires concernant les régiments de l'Armée écossaise?\n",
        "\n",
        "- What injury did Chris Tomlinson sustain, and how has it affected his training and competition plans?\n",
        "\n",
        "Chris Tomlinson, the Olympic champion and world record holder in the triple jump, sustained an injury to his Achilles tendon during training. This has significantly impacted his training and competition plans, as he was set to compete in the European Indoor Championships in Madrid this weekend. According to the article, Tomlinson has been advised to rest for six to eight weeks, which means he will miss the indoor season entirely. This is a significant setback for Tomlinson, as he was in good form leading up to the injury and was looking forward to defending his European indoor title. The article notes that Tomlinson's coach, Ron Morris, is optimistic that he will return to full fitness by the summer, but this will likely mean missing the early part of the outdoor season.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}